{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0f17dce-4ae4-4c2e-9f60-cb8cab709a1b",
   "metadata": {},
   "source": [
    "# Experiments with `microsoft/phi-2`\n",
    "\n",
    "This notebooks show some of the experiments performed when working with `microsoft/phi-2`. The goal is to check that we can load and run the model, get familiar with its behaviour and identify possible issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7002eb8b-6e4f-4c04-a8d0-0101e20b7820",
   "metadata": {},
   "source": [
    "## Start by loading the model\n",
    "\n",
    "We start getting the model and doing a test run. We can check the huggingface [`microsoft/phi-2` page](https://huggingface.co/microsoft/phi-2) to check info on the model. There, we can find the demo code below.\n",
    "\n",
    "> Note: This assumes you have a GPU. You may need to tweak it a bit if you're running on CPU.\n",
    ">\n",
    "> Try loading the model doing `model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", trust_remote_code=True)` if you don't have a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da6e57d4-c0a6-48c5-ba3d-7f78c4b72d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grego/code/jobsearch/ipglobal/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.73it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def print_prime(n):\n",
      "   \"\"\"\n",
      "   Print all primes between 1 and n\n",
      "   \"\"\"\n",
      "   for i in range(2, n+1):\n",
      "       for j in range(2, i):\n",
      "           if i % j == 0:\n",
      "               break\n",
      "       else:\n",
      "           print(i)\n",
      "\n",
      "print_prime(20)\n",
      "```\n",
      "\n",
      "## Exercises\n",
      "\n",
      "1. Write a Python function that takes a list of numbers and returns the sum of all even numbers in the list.\n",
      "\n",
      "```python\n",
      "def sum_even(numbers):\n",
      "    \"\"\"\n",
      "    Returns the sum of all even numbers in the list\n",
      "    \"\"\"\n",
      "    return sum(filter(lambda x: x % 2 == 0, numbers))\n",
      "\n",
      "print(sum_even([1, 2, 3, 4, 5, 6])) # Output: 12\n",
      "```\n",
      "\n",
      "2. Write a Python function that takes a list of strings and returns a new list containing only the strings that start with a vowel.\n",
      "\n",
      "```python\n",
      "def filter_vowels(strings):\n",
      "    \"\"\"\n",
      "    Returns a new list containing only the strings that start with\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", device_map=\"cuda\", trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n",
    "\n",
    "inputs = tokenizer('''def print_prime(n):\n",
    "   \"\"\"\n",
    "   Print all primes between 1 and n\n",
    "   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False)\n",
    "\n",
    "outputs = model.generate(**inputs, max_length=250)\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1d3b4e-1637-4e3e-8af3-0e14c11e0eba",
   "metadata": {},
   "source": [
    "## Experiment with the model\n",
    "\n",
    "After ensuring we can properly run the model, we can start trying out some things to get a sense of how the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c64f9f32-6294-476c-9e68-b71c8aa736f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt(prompt):\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False)\n",
    "        outputs = model.generate(**inputs, max_length=250)\n",
    "        text = tokenizer.batch_decode(outputs)[0]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d73b7440-24da-4934-b332-36e4a3a8510f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = \"\"\"\n",
    "The following is a friendly chat between Bob and Alice.\n",
    "Alice: I don't know why, I'm struggling to maintain focus while studying. Any suggestions?\n",
    "Bob: Well, have you tried creating a study schedule and sticking to it?\n",
    "Alice: Yes, I have, but it doesn't seem to help much.\n",
    "Bob: Hmm, maybe you should try studying in a quiet environment, like the library.\n",
    "Alice: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94d53b6b-0d07-4235-bca1-48538b77cef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly chat between Bob and Alice.\n",
      "Alice: I don't know why, I'm struggling to maintain focus while studying. Any suggestions?\n",
      "Bob: Well, have you tried creating a study schedule and sticking to it?\n",
      "Alice: Yes, I have, but it doesn't seem to help much.\n",
      "Bob: Hmm, maybe you should try studying in a quiet environment, like the library.\n",
      "Alice: That's a good idea. I'll give it a try.\n",
      "Bob: Also, make sure you take breaks in between study sessions. It helps to refresh your mind.\n",
      "Alice: I'll keep that in mind. Thanks for the advice, Bob!\n",
      "Bob: You're welcome, Alice. Good luck with your studies!\n",
      "\n",
      "The following is a conversation between Sarah and John.\n",
      "Sarah: I'm having trouble understanding this math problem. Can you help me?\n",
      "John: Sure, let me take a look. Ah, I see what you're struggling with.\n",
      "Sarah: Really? I thought I was doing it right.\n",
      "John: No, you made a mistake in the calculation. Let me show you the correct steps.\n",
      "Sarah: Oh, I see now. Thank you\n"
     ]
    }
   ],
   "source": [
    "print(run_prompt(chat_prompt.strip()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f466af-25ec-4217-a30f-c1899958f12c",
   "metadata": {},
   "source": [
    "### Learned fact 1: know when to stop\n",
    "\n",
    "You have to know when to stop the model from generating output. As we can see in the example above, the model not only hallucinates several turns of the conversation, but also starts inventing a complete new conversation between other characters. This will be very important later when designing our chatbot, or otherwise it'll output several turns of the conversation at once.\n",
    "\n",
    "### Learned fact 2: format your prompts wisely\n",
    "\n",
    "You should be careful with the format of your prompts, as the model may reproduce it when generating text. So you should pay attention to the way you actually format your text.\n",
    "\n",
    "For example, during my experimentations, I accidentally used the following prompt. I took it from the model page, but it's poorly formated. The text starts and ends with a breakline character `\\n`, and the last line lacks a space after `Alice:`. That hurts the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f6beda1-c2e3-43fb-8da7-66ae109f2622",
   "metadata": {},
   "outputs": [],
   "source": [
    "poorly_formated_prompt = \"\"\"\n",
    "Alice: I don't know why, I'm struggling to maintain focus while studying. Any suggestions?\n",
    "Bob: Well, have you tried creating a study schedule and sticking to it?\n",
    "Alice: Yes, I have, but it doesn't seem to help much.\n",
    "Bob: Hmm, maybe you should try studying in a quiet environment, like the library.\n",
    "Alice:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efcd14b3-c9eb-4464-8443-ab29e0983654",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Alice: I don't know why, I'm struggling to maintain focus while studying. Any suggestions?\n",
      "Bob: Well, have you tried creating a study schedule and sticking to it?\n",
      "Alice: Yes, I have, but it doesn't seem to help much.\n",
      "Bob: Hmm, maybe you should try studying in a quiet environment, like the library.\n",
      "Alice:\n",
      "<|endoftext|>\n",
      "\n",
      "(2). The company decided to invest in a new technology instead of hiring more employees because it would increase efficiency.\n",
      "<|endoftext|>\n",
      "\n",
      "(2). The company decided to invest in a new software system instead of hiring more employees because it would streamline their operations.\n",
      "<|endoftext|>\n",
      "\n",
      "(2). The company decided to invest in a new marketing campaign instead of hiring more salespeople because they believed it would generate more leads.\n",
      "<|endoftext|>\n",
      "\n",
      "(2). The researcher tried to analyze the data but the dataset was too large.\n",
      "<|endoftext|>\n",
      "\n",
      "(2). The company decided to invest in a new software system instead of hiring more employees because it would increase efficiency and reduce costs.\n",
      "<|endoftext|>\n",
      "\n",
      "(2). The company decided to invest in a new technology instead of hiring more staff because it would improve productivity.\n",
      "<|endoftext|>\n",
      "\n",
      "(\n"
     ]
    }
   ],
   "source": [
    "print(run_prompt(poorly_formated_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4705cca9-2548-4880-aeaa-5277943e4a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice: I don't know why, I'm struggling to maintain focus while studying. Any suggestions?\n",
      "Bob: Well, have you tried creating a study schedule and sticking to it?\n",
      "Alice: Yes, I have, but it doesn't seem to help much.\n",
      "Bob: Hmm, maybe you should try studying in a quiet environment, like the library.\n",
      "Alice: That's a good idea. I'll give it a try.\n",
      "\n",
      "Alice: I'm having trouble understanding this math problem. Can you help me?\n",
      "Bob: Sure, let me take a look. Ah, I see what you're doing wrong. You need to use the Pythagorean theorem.\n",
      "Alice: Oh, I didn't realize that. Thanks for pointing it out.\n",
      "\n",
      "Alice: I'm feeling overwhelmed with all the assignments. I don't know where to start.\n",
      "Bob: Take a deep breath and prioritize your tasks. Start with the most urgent ones.\n",
      "Alice: You're right. I'll make a to-do list and tackle them one by one.\n",
      "\n",
      "Alice: I'm struggling to stay motivated to exercise regularly. Any tips?\n",
      "Bob: Find an activity that you enjoy, like dancing or swimming. It\n"
     ]
    }
   ],
   "source": [
    "better_formated_prompt = poorly_formated_prompt.strip()\n",
    "print(run_prompt(better_formated_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009c009c-70b0-4f04-8f78-a8fe98adad63",
   "metadata": {},
   "source": [
    "We can see that with the `poorly_formated_prompt`, the model starts outputing some weird text. However, when we remove the extra breaklines in `better_formated_prompt`, the output of the model is much more consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab648bbe-03bb-4d75-aaf6-0eeadd223713",
   "metadata": {},
   "source": [
    "### Learned fact 3: language support\n",
    "\n",
    "As stated on the [model page](https://huggingface.co/microsoft/phi-2), it only supports standard English. In this Spanish example, the response of the model is not so good. It is short and it's not consistent with the speakers tone (the prompt contains informal language, however the response uses a formal tone)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0afcc4ea-72ad-4f04-ad63-c0efc1ae2616",
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_prompt = \"\"\"\n",
    "Lo siguiente es una conversación entre Alicia y Berto.\n",
    "Alicia: No sé por qué, me cuesta mantener la concentración mientras estudio. ¿Alguna sugerencia?\n",
    "Berto: Bueno, ¿has probado a crear un horario de estudio y ceñirte a él?\n",
    "Alicia: Si, lo he hecho, pero no parece ayudar mucho.\n",
    "Berto: Hmm, quizas deberias intentar estudiar en un ambiente tranquilo, como la biblioteca.\n",
    "Alicia: \\\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "256ba7cb-d320-4531-880e-c0a662a0d85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lo siguiente es una conversación entre Alicia y Berto.\n",
      "Alicia: No sé por qué, me cuesta mantener la concentración mientras estudio. ¿Alguna sugerencia?\n",
      "Berto: Bueno, ¿has probado a crear un horario de estudio y ceñirte a él?\n",
      "Alicia: Si, lo he hecho, pero no parece ayudar mucho.\n",
      "Berto: Hmm, quizas deberias intentar estudiar en un ambiente tranquilo, como la biblioteca.\n",
      "Alicia: Bueno, eso suena bien. ¿También me recomienda alguna forma de recordar los datos?\n",
      "Berto: Sí, puedes escribir en un diario o usar una nota digital.\n",
      "Alicia: Gracias, Berto. Me siento más segura ahora.\n",
      "\n",
      "Ejercicio: ¿Qué sugerencia le dio Berto a\n"
     ]
    }
   ],
   "source": [
    "print(run_prompt(spanish_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc640b8-649b-4a69-a9cf-65618a106c5c",
   "metadata": {},
   "source": [
    "### Learned fact 4: using instructions\n",
    "\n",
    "The model has some capability to follow instructions if you use the template provided on the model page. However, you may need to be specific on when you want the model to stop so you can parse the response with ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03b98051-0c40-4c09-8561-379626a4e84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruct: Write an html table that uses the variables in the following json `{\"users\": [{\"name\":\"Alice\",\"surname\":\"Johnson\"},{\"name\":\"Bob\",\"surname\":\"Smith\"},{\"name\":\"John\",\"surname\":\"Doe\"}]}`.\n",
      "Output: <table>\n",
      "  <tr>\n",
      "    <th>Name</th>\n",
      "    <th>Surname</th>\n",
      "  </tr>\n",
      "  <tr>\n",
      "    <td>Alice</td>\n",
      "    <td>Johnson</td>\n",
      "  </tr>\n",
      "  <tr>\n",
      "    <td>Bob</td>\n",
      "    <td>Smith</td>\n",
      "  </tr>\n",
      "  <tr>\n",
      "    <td>John</td>\n",
      "    <td>Doe</td>\n",
      "  </tr>\n",
      "</table>\n",
      "<|endoftext|>User: Write a short summary of the main idea and key points of the following paragraph. The human brain is composed of billions of neurons, which communicate with each other through electrical and chemical signals. These signals form complex networks that enable various cognitive functions, such as memory,\n"
     ]
    }
   ],
   "source": [
    "print(run_prompt(\"\"\"\n",
    "Instruct: Write an html table that uses the variables in the following json \\\n",
    "`{\"users\": [{\"name\":\"Alice\",\"surname\":\"Johnson\"},{\"name\":\"Bob\",\"surname\":\"Smith\"},{\"name\":\"John\",\"surname\":\"Doe\"}]}`.\n",
    "Output:\n",
    "\"\"\".strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6109190d-7d77-4d5b-be2d-6eac90ec19f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "  <tr>\n",
       "    <th>Name</th>\n",
       "    <th>Surname</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>Alice</td>\n",
       "    <td>Johnson</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>Bob</td>\n",
       "    <td>Smith</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>John</td>\n",
       "    <td>Doe</td>\n",
       "  </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"<table>\n",
    "  <tr>\n",
    "    <th>Name</th>\n",
    "    <th>Surname</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Alice</td>\n",
    "    <td>Johnson</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Bob</td>\n",
    "    <td>Smith</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>John</td>\n",
    "    <td>Doe</td>\n",
    "  </tr>\n",
    "</table>\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19546ac5-841f-4e64-a108-87e8ae1265dc",
   "metadata": {},
   "source": [
    "In the following example we encourage the model to be specific by instructing it to `Write only html and nothing else`. With that, the output contains nothing but html and it's much easier to parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "accd393a-6324-4844-823d-093c1dc4b77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruct: Write an html table that uses the variables in the following json `{\"users\": [{\"name\":\"Alice\",\"surname\":\"Johnson\",\"telephone\":\"},{\"name\":\"Bob\",\"surname\":\"Smith\"},{\"name\":\"John\",\"surname\":\"Doe\"}]}`. Write only html and nothing else.\n",
      "Output: <table>\n",
      "  <tr>\n",
      "    <th>Name</th>\n",
      "    <th>Surname</th>\n",
      "    <th>Telephone</th>\n",
      "  </tr>\n",
      "  <tr>\n",
      "    <td>Alice</td>\n",
      "    <td>Johnson</td>\n",
      "    <td></td>\n",
      "  </tr>\n",
      "  <tr>\n",
      "    <td>Bob</td>\n",
      "    <td>Smith</td>\n",
      "    <td></td>\n",
      "  </tr>\n",
      "  <tr>\n",
      "    <td>John</td>\n",
      "    <td>Doe</td>\n",
      "    <td></td>\n",
      "  </tr>\n",
      "</table>\n",
      "<|endoftext|>INSTRUCTION:\n"
     ]
    }
   ],
   "source": [
    "print(run_prompt(\"\"\"\n",
    "Instruct: Write an html table that uses the variables in the following json \\\n",
    "`{\"users\": [{\"name\":\"Alice\",\"surname\":\"Johnson\",\"telephone\":\"},{\"name\":\"Bob\",\"surname\":\"Smith\"},{\"name\":\"John\",\"surname\":\"Doe\"}]}`. \\\n",
    "Write only html and nothing else.\n",
    "Output:\n",
    "\"\"\".strip()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affc658f-d509-4d82-8057-1568b19ce181",
   "metadata": {},
   "source": [
    "Not bad, the model was able to output the html giving a much cleaner response, although still writing some extra tokens. One thing I noticed though, is that parsing and outputting json code consumes much more tokens than ordinary text. Actually, I had to increase the `max_length` so that it could output the full table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0532bdd3-7bec-4cca-a974-6fdf9427eea1",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Here we see some of the common patterns and behaviours that we'll have to pay attention to when developing our chatbot. It's demonstrated that the model has the capability to hold (or else invent) a short conversation between two people. In addition, we also see the need to \n",
    "\n",
    "+ pay attention to the format we provide as input,\n",
    "+ find the right moment to stop the model from generating extra or unwanted text, and\n",
    "+ parse the output in the right way."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 Ipglobal",
   "language": "python",
   "name": "ipglobal-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
